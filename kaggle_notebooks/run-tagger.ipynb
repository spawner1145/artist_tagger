{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-12T07:23:19.440138Z",
     "iopub.status.busy": "2025-01-12T07:23:19.439769Z",
     "iopub.status.idle": "2025-01-12T07:23:19.444679Z",
     "shell.execute_reply": "2025-01-12T07:23:19.443878Z",
     "shell.execute_reply.started": "2025-01-12T07:23:19.440106Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, average_precision_score\n",
    "import warnings\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T07:23:19.446000Z",
     "iopub.status.busy": "2025-01-12T07:23:19.445794Z",
     "iopub.status.idle": "2025-01-12T07:23:19.469710Z",
     "shell.execute_reply": "2025-01-12T07:23:19.469152Z",
     "shell.execute_reply.started": "2025-01-12T07:23:19.445982Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_dir = 'D:/tagger/artist_tagger/train_example_folder'  # tags.csv所在的文件夹\n",
    "# 预测目录中的图像\n",
    "img_dir = 'D:/tagger/artist_tagger/input_images'  # 替换为要预测的图片所在的目录\n",
    "# 模型路径\n",
    "model_path = 'D:/tagger/artist_tagger/tagger.pth'\n",
    "# 设置阈值，不支持使用自动阈值, 输入0到1的阈值\n",
    "threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T07:23:19.471188Z",
     "iopub.status.busy": "2025-01-12T07:23:19.470966Z",
     "iopub.status.idle": "2025-01-12T07:23:19.482789Z",
     "shell.execute_reply": "2025-01-12T07:23:19.482236Z",
     "shell.execute_reply.started": "2025-01-12T07:23:19.471170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 忽略特定警告\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def convert_images_to_jpg(input_dir):\n",
    "    supported_formats = ['.bmp', '.tiff', '.webp', '.gif']\n",
    "    for filename in os.listdir(input_dir):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        if filename.lower().endswith('.jpg'):\n",
    "            continue\n",
    "        try:\n",
    "            if filename.lower().endswith('.gif'):\n",
    "                cap = cv2.VideoCapture(file_path)\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    output_path = os.path.splitext(file_path)[0] + '.jpg'\n",
    "                    cv2.imwrite(output_path, cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                cap.release()\n",
    "                os.remove(file_path)\n",
    "            elif filename.lower().endswith(tuple(supported_formats)):\n",
    "                img = Image.open(file_path).convert(\"RGB\")\n",
    "                output_path = os.path.splitext(file_path)[0] + '.jpg'\n",
    "                img.save(output_path, \"JPEG\")\n",
    "                img.close()\n",
    "                if file_path != output_path:\n",
    "                    os.remove(file_path)\n",
    "            else:\n",
    "                print(f\"Unsupported file type: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "class ArtStyleDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None, mlb=None):\n",
    "        self.annotations = pd.read_csv(csv_file, encoding='latin1')\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.mlb = mlb\n",
    "        # 将字符串格式的列表转换回列表\n",
    "        self.annotations['tags'] = self.annotations['tags'].apply(lambda x: x.split(','))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        try:\n",
    "            image = Image.open(img_name).convert(\"RGB\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error loading image {img_name}: {e}\")\n",
    "            return None\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        labels = self.annotations.iloc[index]['tags']\n",
    "        labels = self.mlb.transform([labels])[0]  # 直接传入标签列表\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))  # 过滤掉None值\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T07:23:19.483769Z",
     "iopub.status.busy": "2025-01-12T07:23:19.483526Z",
     "iopub.status.idle": "2025-01-12T07:23:19.535502Z",
     "shell.execute_reply": "2025-01-12T07:23:19.534748Z",
     "shell.execute_reply.started": "2025-01-12T07:23:19.483744Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported file type: 002_03.png\n",
      "Unsupported file type: 007_01.png\n",
      "Unsupported file type: 025_04.png\n",
      "Unsupported file type: 1.png\n",
      "Unsupported file type: 007_03.png\n",
      "Unsupported file type: 007_02.png\n",
      "Unsupported file type: 007_04.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiLabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>MultiLabelBinarizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiLabelBinarizer()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class SpatialAttentionModule(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttentionModule, self).__init__()\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class MultiLayerAttentionResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiLayerAttentionResNet, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # 提取 ResNet 的各层\n",
    "        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        \n",
    "        # 添加 SE Block 和空间注意力模块到每一层\n",
    "        self.se_block1 = SEBlock(channel=self.layer1[-1].conv3.out_channels)\n",
    "        self.spatial_attention1 = SpatialAttentionModule()\n",
    "        \n",
    "        self.se_block2 = SEBlock(channel=self.layer2[-1].conv3.out_channels)\n",
    "        self.spatial_attention2 = SpatialAttentionModule()\n",
    "        \n",
    "        self.se_block3 = SEBlock(channel=self.layer3[-1].conv3.out_channels)\n",
    "        self.spatial_attention3 = SpatialAttentionModule()\n",
    "        \n",
    "        self.se_block4 = SEBlock(channel=self.layer4[-1].conv3.out_channels)\n",
    "        self.spatial_attention4 = SpatialAttentionModule()\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # 添加多层感知机（MLP）\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(resnet.fc.in_features, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5)\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.se_block1(x)\n",
    "        x = x * self.spatial_attention1(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.se_block2(x)\n",
    "        x = x * self.spatial_attention2(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = self.se_block3(x)\n",
    "        x = x * self.spatial_attention3(x)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        x = self.se_block4(x)\n",
    "        x = x * self.spatial_attention4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.mlp(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# 图像格式转换\n",
    "convert_images_to_jpg(img_dir)\n",
    "\n",
    "# 加载标签信息并初始化 MultiLabelBinarizer\n",
    "styles_df = pd.read_csv(os.path.join(base_dir, 'tags.csv'), encoding='latin1')  # 确保正确的编码\n",
    "styles = styles_df['tag'].tolist()  # 确保列名正确\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([[style] for style in styles])  # 每个标签作为一个单独的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T07:23:19.536698Z",
     "iopub.status.busy": "2025-01-12T07:23:19.536427Z",
     "iopub.status.idle": "2025-01-12T07:23:20.295720Z",
     "shell.execute_reply": "2025-01-12T07:23:20.295025Z",
     "shell.execute_reply.started": "2025-01-12T07:23:19.536671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_workers = 4 if torch.cuda.is_available() else 0\n",
    "pin_memory = True if torch.cuda.is_available() else False\n",
    "\n",
    "model = MultiLayerAttentionResNet(num_classes=len(styles))\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T07:23:20.297259Z",
     "iopub.status.busy": "2025-01-12T07:23:20.296932Z",
     "iopub.status.idle": "2025-01-12T07:23:21.096759Z",
     "shell.execute_reply": "2025-01-12T07:23:21.096050Z",
     "shell.execute_reply.started": "2025-01-12T07:23:20.297227Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-94629d91ffdb>:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully.\n",
      "Predictions for 002_03.png: {}\n",
      "Predictions for 007_01.png: {}\n",
      "Predictions for 025_04.png: {}\n",
      "Predictions for 1.png: {}\n",
      "Predictions for 007_03.png: {}\n",
      "Predictions for 007_02.png: {}\n",
      "Predictions for 007_04.png: {}\n",
      "Classes: ['.com (bot com1)' '.sin' '0 (znanimo)' ... 'zzb azz' 'zzo0' 'zzyzzyy']\n"
     ]
    }
   ],
   "source": [
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def predict_image_styles(img_path, model, transform, label_encoder, threshold, device=device):\n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening image {img_path}: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    \n",
    "    image = image.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probabilities = torch.sigmoid(outputs)  # 使用sigmoid函数将输出转换为概率\n",
    "        probabilities = probabilities.cpu().numpy().flatten()\n",
    "\n",
    "    style_prob_dict = {label: prob for label, prob in zip(label_encoder.classes_, probabilities) if prob > threshold}\n",
    "\n",
    "    sorted_style_prob_dict = dict(sorted(style_prob_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return sorted_style_prob_dict\n",
    "\n",
    "\n",
    "# 加载模型\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model weights: {e}\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for filename in os.listdir(img_dir):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
    "        img_path = os.path.join(img_dir, filename)\n",
    "        predictions = predict_image_styles(img_path, model, transform_val, mlb, threshold, device)\n",
    "        print(f\"Predictions for {filename}: {predictions}\")\n",
    "\n",
    "print(\"Classes:\", mlb.classes_)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6467238,
     "sourceId": 10448231,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
